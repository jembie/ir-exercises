{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96fd1f99",
   "metadata": {},
   "source": [
    "# Natural Language Processing with nltk & spaCy\n",
    "\n",
    "<div>\n",
    "<img src=\"../../images/lab02/spacy_landingpage.png\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "_(Adapted from [spacy](https://spacy.io/usage/spacy-101))_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b707d09b",
   "metadata": {},
   "source": [
    "# 1. Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31996d6",
   "metadata": {},
   "source": [
    "Before we begin, we need to download some extra tools.\n",
    "\n",
    "First, Update your virtual environment from last time and include the new dependencies by writing the following in your terminal:\n",
    "\n",
    "```bash\n",
    "uv sync\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "Next, download the spacy model that we will work with:\n",
    "\n",
    "```bash\n",
    "uv run --with pip spacy download en_core_web_lg\n",
    "\n",
    "# Alternatively while being in the virtual environment:\n",
    "python -m spacy download en_core_web_lg\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebcf7f2",
   "metadata": {},
   "source": [
    "The **spaCy** library provides a variety of linguistic annotations to give the user insights into the grammatical structure of a text snippet. This includes the word types, like the parts of speech, and how the words are related to each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d152c438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple PROPN NNP nsubj Xxxxx True False\n",
      "is be AUX VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. U.K. PROPN NNP dobj X.X. False False\n",
      "startup startup VERB VB advcl xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from typing import Set, Literal, Tuple\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4b8aa0",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"../../images/lab02/spacy_pos.png\" width=\"1000\"/>\n",
    "</div>\n",
    "\n",
    "- **`Text`**: The original token text\n",
    "- **`Lemma`**: The base form of the token\n",
    "- **`POS`**: The simple **Universal Part of Speech** tags\n",
    "- **`Tag`**: The detailed part-of-speech tag\n",
    "- **`Dep`**: Syntactic dependency, meaning the relationship between tokens.\n",
    "- **`Shape`**: The word shape, i.e. capitalization, punctuation, digits -**`is alpha`**: Does the token consist of alphabetic characters?\n",
    "- **`is stop`**: Is the token part of a stop list?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "342412c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The the DET DT det Xxx True True\n",
      "Dresden Dresden PROPN NNP compound Xxxxx True False\n",
      "                SPACE _SP dep      False False\n",
      "University University PROPN NNP nsubjpass Xxxxx True False\n",
      "of of ADP IN prep xx True True\n",
      "Technology Technology PROPN NNP pobj Xxxxx True False\n",
      "( ( PUNCT -LRB- punct ( False False\n",
      "the the DET DT det xxx True True\n",
      "Collaborative Collaborative PROPN NNP compound Xxxxx True False\n",
      "University University PROPN NNP compound Xxxxx True False\n",
      "ðŸ˜ƒ ðŸ˜ƒ X FW appos ðŸ˜ƒ False False\n",
      ") ) PUNCT -RRB- punct ) False False\n",
      "is be AUX VBZ auxpass xx True True\n",
      "known know VERB VBN ROOT xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "their their PRON PRP$ poss xxxx True True\n",
      "excellence excellence NOUN NN pobj xxxx True False\n",
      ". . PUNCT . punct . False False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"The Dresden        University of Technology (the Collaborative University ðŸ˜ƒ) is known for their excellence.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8c82b5",
   "metadata": {},
   "source": [
    "## In a Nutshell: how does the Tokenization process work?\n",
    "\n",
    "As we know from the previous exercise, **tokenization** is the task of splitting a text into meaningful segments, which are also known as **tokens**. One important detail here is: spaCy's tokenization is **non-destructive**, meaning that we can always reconstruct the original input from the tokenized output. Information such as **whitespaces** is preserved in the tokens and no additional information is neither added nor removed during tokenization.\n",
    "\n",
    "<div>\n",
    "<img src=\"../../images/lab02/spacy_tokenization.png\" width=\"1000\"/>\n",
    "</div>\n",
    "\n",
    "1. In the very first step, the raw text is split on whitespace characters, similar to how **`text.split(' ')`** acts.\n",
    "2. Next, the tokenizer processes the text **from left to right**, performing two checks on each substring:\n",
    "   <br></br>\n",
    "   - **Does the current substring match a tokenizer exception rule?**: An example for this is: \"**`don't`**\" has no whitespaces, however, it should still be split into the two tokens \"do\" and \"n't\", while something like \"U.K.\" should always remain as one token.\n",
    "   - **Can a prefix, suffix or infix be split off?**: For example punctuation like commas, periods, hyphens or quotes.\n",
    "\n",
    "If we have a match, the rule is applied and the tokenizer continues the loop, moving to the next substring.\n",
    "\n",
    "<div>\n",
    "<img src=\"../../images/lab02/spacy_tokenization_visualised.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "Important to note here is that the tokenizer exceptions strongly depend on the specifics of the individual language, so one must always load the correct subclass model to maximize performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ca5f9e",
   "metadata": {},
   "source": [
    "We can also easily make our own special cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fd709cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gimme', 'that']\n"
     ]
    }
   ],
   "source": [
    "from spacy.symbols import ORTH\n",
    "\n",
    "doc = nlp(\"gimme that\")\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dccdd76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gim', 'me', 'that']\n"
     ]
    }
   ],
   "source": [
    "# Create the special case rule\n",
    "special_case = [{ORTH: \"gim\"}, {ORTH: \"me\"}]\n",
    "nlp.tokenizer.add_special_case(\"gimme\", special_case)\n",
    "\n",
    "print([token.text for token in nlp(\"gimme that\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f839fa",
   "metadata": {},
   "source": [
    "## 2. Now, we will apply the knowledge from the previous lectures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dc9ec4",
   "metadata": {},
   "source": [
    "First we need to download a dataset, this one \"only\" has 3.000.000 entries (the original had ~30M rows resulting in a total size of 20 GiB; If any of you has enough RAM - and the patience - to work with the full dataset, see [here](https://ir-datasets.com/car.html#car/v1.5))\n",
    "\n",
    "You can decide how many rows you wish to have with choosing a value **`nrows`**, where\n",
    "$0 \\lt n_{rows} \\leq 3.000.000$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14058e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000000e7e72cafb61a9f356b7dceb25c5e028db</td>\n",
       "      <td>Ukraine was one of the most dangerous places f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000005eafb11cbd7e9780161f9f0a0a6fd897cd</td>\n",
       "      <td>'''October 4'''  Missouri opened its season wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000000afe1da525b3db17db77f350b187441a9ed</td>\n",
       "      <td>The 1913 Johannisthal Air Disaster happened cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000014f07ec8c3f6dc130d21cddc42da950068a</td>\n",
       "      <td>StÃ©phane Houdet successfully defended the titl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000001ae0be1235060c10e6edc99f7791e86a04c</td>\n",
       "      <td>Ayscough was murdered on 29 June 1450 by an an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0002120669ce77dd3655cbc52a5f98ac29cc9d39</td>\n",
       "      <td>Today the hanger continues to serve the needs ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>000212a87625c9e6e9b8ff6e1c63a4a3691c2b66</td>\n",
       "      <td>Before Homer's Trojan War, Heracles had made a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>000212f81a51f377dbe02e0f41e0c27d135f0cb8</td>\n",
       "      <td>He had devoted himself since graduation to lit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>000213593c014454cefea0d5ce2ea2af964beda1</td>\n",
       "      <td>In 1975, he was the starter for the full seaso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0002138189e61bbc5b5b8a274aa76361c8c09c20</td>\n",
       "      <td>Both major parties finished with around 35% ea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       doc_id  \\\n",
       "0    0000000e7e72cafb61a9f356b7dceb25c5e028db   \n",
       "1    0000005eafb11cbd7e9780161f9f0a0a6fd897cd   \n",
       "2    000000afe1da525b3db17db77f350b187441a9ed   \n",
       "3    0000014f07ec8c3f6dc130d21cddc42da950068a   \n",
       "4    000001ae0be1235060c10e6edc99f7791e86a04c   \n",
       "..                                        ...   \n",
       "995  0002120669ce77dd3655cbc52a5f98ac29cc9d39   \n",
       "996  000212a87625c9e6e9b8ff6e1c63a4a3691c2b66   \n",
       "997  000212f81a51f377dbe02e0f41e0c27d135f0cb8   \n",
       "998  000213593c014454cefea0d5ce2ea2af964beda1   \n",
       "999  0002138189e61bbc5b5b8a274aa76361c8c09c20   \n",
       "\n",
       "                                                  text  \n",
       "0    Ukraine was one of the most dangerous places f...  \n",
       "1    '''October 4'''  Missouri opened its season wi...  \n",
       "2    The 1913 Johannisthal Air Disaster happened cl...  \n",
       "3    StÃ©phane Houdet successfully defended the titl...  \n",
       "4    Ayscough was murdered on 29 June 1450 by an an...  \n",
       "..                                                 ...  \n",
       "995  Today the hanger continues to serve the needs ...  \n",
       "996  Before Homer's Trojan War, Heracles had made a...  \n",
       "997  He had devoted himself since graduation to lit...  \n",
       "998  In 1975, he was the starter for the full seaso...  \n",
       "999  Both major parties finished with around 35% ea...  \n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv(\"hf://datasets/jembie/carv1.5_reduced/carv1.5_reduced.csv\", nrows=1_000)\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f82172",
   "metadata": {},
   "source": [
    "## What are we working with?\n",
    "\n",
    "<div>\n",
    "<img src=\"../../images/lab02/trec_homepage.png\" width=\"1000\"/>\n",
    "</div>\n",
    "\n",
    "TREC (Text REtrieval Conference) is a long-running research initiative that evaluates and advances information retrieval systems by organizing shared tasks and datasets. The [TREC 2017](https://trec-car.cs.unh.edu/) focuses on:\n",
    "\n",
    "\"This track encourages research for answering more complex information needs with longer answers. Much like Wikipedia pages synthesize knowledge that is globally distributed, we envision systems that collect relevant information from an entire corpus, creating synthetically structured documents by collating retrieved results.\"\n",
    "\n",
    "#### Task:\n",
    "\n",
    "We want to preprocess our dataset so that each row of the DataFrame contains:\n",
    "\n",
    "- the **`document ID`** (doc_id),\n",
    "- the original **`text`** (text),\n",
    "- a list of valid **`tokens`** derived from that text (tokens), that are part of the English Vocbularly (english_vocabulary),\n",
    "- and the total number of those tokens (tokens_count).\n",
    "\n",
    "For this, use spaCyâ€™s English processing pipeline to tokenize and lemmatize the text in each entry of the text column. The related documentation can (hopefully) help you out in this process: [spaCy processing pipelines](https://spacy.io/usage/processing-pipelines/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ae3654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Load large English model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "vocab = nlp.vocab\n",
    "\n",
    "english_vocabulary = set(word for word in vocab.strings)\n",
    "\n",
    "\n",
    "def preprocess_pipeline(texts, english_vocab) -> Tuple[..., ...]:\n",
    "    for doc in nlp.pipe(texts, batch_size=50, n_process=4):\n",
    "        # Only permit tokens that are in the english vocabulary, not a stop work, and numeric\n",
    "        ...\n",
    "\n",
    "    return ..., ...\n",
    "\n",
    "\n",
    "# Apply preprocessing\n",
    "\n",
    "data[\"tokens\"] = ...\n",
    "data[\"tokens_count\"] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884b1e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can inspect the columns here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e74f3c",
   "metadata": {},
   "source": [
    "#### Some basic post-processing\n",
    "\n",
    "Since we have a very simple pipeline it is still possible that our tokenizer made issues during the process. Get rid of the entries where the tokenization failed, i.e. we have an empty list in the tokens column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1816f5b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbf189d4",
   "metadata": {},
   "source": [
    "### 3. Build the Inverted Index\n",
    "\n",
    "Build the inverted index, where the key (or the index of the DataFrame, if you prefer to use pandas) represents each term we have collected, mapping it to a list of corresponding document IDs. Although our current model is not yet capable of ranking by relevance (since we lack the ability to measure numerically of how \"important\" a piece of text is for a given query) we still want to collect useful metadata such as **`Document Frequency`** and **`Total Term Frequency`** for our future improvements. For now, we do not need to store the frequency of each term within individual documents; in other words, our Postingslists will simply contain the list of document IDs.\n",
    "\n",
    "**You must implement the Inverted Index with a Document Frequency entry for each term. The Total Term Frequency entry is optional for this lab.**\n",
    "<br></br>\n",
    "As a quick reminder, here is the inverted index structure:\n",
    "\n",
    "<div>\n",
    "<img src=\"../../images/lab02/Inverted_Index.svg\">\n",
    "</div>\n",
    "\n",
    "(_Adapted from_ [_De Paul University CSC575_](http://facweb.cs.depaul.edu/mobasher/classes/CSC575/Assignments/assign1-2023.html))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fcc4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perhaps you might create the very basic mapping of (terms -> doc_ids)\n",
    "# via a defaultdict and then add the meta information when transforming\n",
    "# it into a DataFrame\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb1f9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that the 'terms' columns is the new index of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f4af56",
   "metadata": {},
   "source": [
    "### 4. Implement Boolean Retrieval for our Inverted Index, so we can do: AND, OR & NOT Queries\n",
    "\n",
    "For now we want only pretty simple querying, so for example:\n",
    "\n",
    "```py\n",
    "search(\"Apples are great\", operation=\"AND\")\n",
    "# --> \"apples AND are AND great\" is our query\n",
    "```\n",
    "\n",
    "should query our tokens as follows: `\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c576dee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7ee2a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d10681f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067c89e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
