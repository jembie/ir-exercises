{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96fd1f99",
   "metadata": {},
   "source": [
    "# Natural Language Processing with nltk & spaCy\n",
    "\n",
    "<div>\n",
    "<img src=\"../../images/lab02/spacy_landingpage.png\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "_(Adapted from [spacy](https://spacy.io/usage/spacy-101))_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b707d09b",
   "metadata": {},
   "source": [
    "# 1. Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31996d6",
   "metadata": {},
   "source": [
    "Before we begin, we need to download some extra tools.\n",
    "\n",
    "First, Update your virtual environment from last time and include the new dependencies by writing the following in your terminal:\n",
    "\n",
    "```bash\n",
    "uv sync\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "Next, download the spacy model that we will work with:\n",
    "\n",
    "```bash\n",
    "uv run --with pip spacy download en_core_web_lg\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebcf7f2",
   "metadata": {},
   "source": [
    "The **spaCy** library provides a variety of linguistic annotations to give the user insights into the grammatical structure of a text snippet. This includes the word types, like the parts of speech, and how the words are related to each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d152c438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from typing import Set, Literal, Tuple\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4b8aa0",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"../../images/lab02/spacy_pos.png\" width=\"1000\"/>\n",
    "</div>\n",
    "\n",
    "- **`Text`**: The original token text\n",
    "- **`Lemma`**: The base form of the token\n",
    "- **`POS`**: The simple **Universal Part of Speech** tags\n",
    "- **`Tag`**: The detailed part-of-speech tag\n",
    "- **`Dep`**: Syntactic dependency, meaning the relationship between tokens.\n",
    "- **`Shape`**: The word shape, i.e. capitalization, punctuation, digits -**`is alpha`**: Does the token consist of alphabetic characters?\n",
    "- **`is stop`**: Is the token part of a stop list?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342412c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"The Dresden  University of Technology (the Collaborative University ðŸ˜ƒ) is known for their excellence.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8c82b5",
   "metadata": {},
   "source": [
    "## In a Nutshell: how does the Tokenization process work?\n",
    "\n",
    "As we know from the previous exercise, **tokenization** is the task of splitting a text into meaningful segments, which are also known as **tokens**. One important detail here is: spaCy's tokenization is **non-destructive**, meaning that we can always reconstruct the original input from the tokenized output. Information such as **whitespaces** is preserved in the tokens and no additional information is neither added nor removed during tokenization.\n",
    "\n",
    "<div>\n",
    "<img src=\"../../images/lab02/spacy_tokenization.png\" width=\"1000\"/>\n",
    "</div>\n",
    "\n",
    "1. In the very first step, the raw text is split on whitespace characters, similar to how **`text.split(' ')`** acts.\n",
    "2. Next, the tokenizer processes the text **from left to right**, performing two checks on each substring:\n",
    "   <br></br>\n",
    "   - **Does the current substring match a tokenizer exception rule?**: An example for this is: \"**`don't`**\" has no whitespaces, however, it should still be split into the two tokens \"do\" and \"n't\", while something like \"U.K.\" should always remain as one token.\n",
    "   - **Can a prefix, suffix or infix be split off?**: For example punctuation like commas, periods, hyphens or quotes.\n",
    "\n",
    "If we have a match, the rule is applied and the tokenizer continues the loop, moving to the next substring.\n",
    "\n",
    "<div>\n",
    "<img src=\"../../images/lab02/spacy_tokenization_visualised.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "Important to note here is that the tokenizer exceptions strongly depend on the specifics of the individual language, so one must always load the correct subclass model to maximize performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ca5f9e",
   "metadata": {},
   "source": [
    "We can also easily make our own special cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd709cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.symbols import ORTH\n",
    "\n",
    "doc = nlp(\"gimme that\")\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccdd76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the special case rule\n",
    "special_case = [{ORTH: \"gim\"}, {ORTH: \"me\"}]\n",
    "nlp.tokenizer.add_special_case(\"gimme\", special_case)\n",
    "\n",
    "print([token.text for token in nlp(\"gimme that\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f839fa",
   "metadata": {},
   "source": [
    "## 2. Now, we will apply the knowledge from the previous lectures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dc9ec4",
   "metadata": {},
   "source": [
    "First we need to download a dataset, this one \"only\" has 3.000.000 entries (the original had ~30M rows resulting in a total size of 20 GiB; If any of you has enough RAM - and the patience - to work with the full dataset, see [here](https://ir-datasets.com/car.html#car/v1.5))\n",
    "\n",
    "You can decide how many rows you wish to have with choosing a value **`nrows`**, where\n",
    "$0 \\lt n_{rows} \\leq 3.000.000$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14058e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"hf://datasets/jembie/carv1.5_reduced/carv1.5_reduced.csv\", nrows=1_000)\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f82172",
   "metadata": {},
   "source": [
    "## What are we working with?\n",
    "\n",
    "<div>\n",
    "<img src=\"../../images/lab02/trec_homepage.png\" width=\"1000\"/>\n",
    "</div>\n",
    "\n",
    "TREC (Text REtrieval Conference) is a long-running research initiative that evaluates and advances information retrieval systems by organizing shared tasks and datasets. The [TREC 2017](https://trec-car.cs.unh.edu/) focuses on:\n",
    "\n",
    "\"This track encourages research for answering more complex information needs with longer answers. Much like Wikipedia pages synthesize knowledge that is globally distributed, we envision systems that collect relevant information from an entire corpus, creating synthetically structured documents by collating retrieved results.\"\n",
    "\n",
    "#### Task:\n",
    "\n",
    "Do some preprocessing on our data, and collect for each entry in the **`text`** column all stemmed and lowercased tokens that are part of the english vocabulary. In the end we want a pandas **`DataFrame`** that has the following column structure when calling **`data.columns`**:\n",
    "\n",
    "```py\n",
    "Index(['doc_id', 'text', 'tokens', 'tokens_count'], dtype='object')\n",
    "```\n",
    "\n",
    "For this, use the related documentation to (hopefully) help you out: [spaCy processing pipelines](https://spacy.io/usage/processing-pipelines/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ae3654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Load large English model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "vocab = nlp.vocab\n",
    "\n",
    "english_vocabulary = set(word for word in vocab.strings)\n",
    "\n",
    "\n",
    "def preprocess_pipeline(texts, english_vocab) -> Tuple[..., ...]:\n",
    "    for doc in nlp.pipe(texts, batch_size=50, n_process=4):\n",
    "        # Only permit tokens that are in the english vocabulary, not a stop work, and numeric\n",
    "        ...\n",
    "\n",
    "    return ..., ...\n",
    "\n",
    "\n",
    "# Apply preprocessing\n",
    "\n",
    "data[\"tokens\"] = ...\n",
    "data[\"tokens_count\"] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884b1e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can inspect the columns here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e74f3c",
   "metadata": {},
   "source": [
    "#### Some basic post-processing\n",
    "\n",
    "Since we have a very simple pipeline it is still possible that our tokenizer made issues during the process. Get rid of the entries where the tokenization failed, i.e. we have an empty list in the tokens column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1816f5b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbf189d4",
   "metadata": {},
   "source": [
    "### 3. Build the Inverted Index\n",
    "\n",
    "Build the inverted index, where the key (or the index of the DataFrame, if you prefer to use pandas) represents each term we have collected, mapping it to a list of corresponding document IDs. Although our current model is not yet capable of ranking by relevance (since we lack the ability to measure numerically of how \"important\" a piece of text is for a given query) we still want to collect useful metadata such as **`Document Frequency`** and **`Total Term Frequency`** for our future improvements. For now, we do not need to store the frequency of each term within individual documents; in other words, our Postingslists will simply contain the list of document IDs.\n",
    "\n",
    "**You must implement the Inverted Index with a Document Frequency entry for each term. The Total Term Frequency entry is optional for this lab.**\n",
    "<br></br>\n",
    "As a quick reminder, here is the inverted index structure:\n",
    "\n",
    "<div>\n",
    "<img src=\"../../images/lab02/Inverted_Index.svg\">\n",
    "</div>\n",
    "\n",
    "(_Adapted from_ [_De Paul University CSC575_](http://facweb.cs.depaul.edu/mobasher/classes/CSC575/Assignments/assign1-2023.html))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fcc4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perhaps you might create the very basic mapping of (terms -> doc_ids)\n",
    "# via a defaultdict and then add the meta information when transforming\n",
    "# it into a DataFrame\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb1f9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that the 'terms' columns is the new index of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f4af56",
   "metadata": {},
   "source": [
    "### 4. Implement Boolean Retrieval for our Inverted Index, so we can do: AND, OR & NOT Queries\n",
    "\n",
    "For now we want only pretty simple querying, so for example:\n",
    "\n",
    "```py\n",
    "search(\"Apples are great\", operation=\"AND\")\n",
    "# --> \"apples AND are AND great\" is our query\n",
    "```\n",
    "\n",
    "should query our tokens as follows: `\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c576dee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7ee2a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d10681f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067c89e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
